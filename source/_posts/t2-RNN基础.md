---
title: t2-RNN基础
mathjax: true
date: 2020-02-14 21:35:24
categories:
- 动手深度学习
tags:
---

## 循环神经网络

RNN网络在t时刻接收到输入$x_t$ 之后，隐藏层的值是$s_t$) ，输出值是$o_t$。关键一点是， $s_t$的值不仅仅取决于取$x_t$, 还决于$s_{t-1}$.我们用下面可以的公式来表示**循环神经网络**的计算方法：
$$
O_t = g(V \cdot S_t) \\
S_t = f(U \cdot X_t + W \cdot S_{t-1})
$$
#### 梯度剪裁

存在 **梯度消失** 和 **梯度爆炸** 问题，所以考虑使用梯度剪裁。

裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 $g$，并设裁剪的阈值是$\theta$。裁剪后的梯度
$$
min(\frac{\theta}{||g||}, 1)g
$$
的$L_2$范数不超过$\theta$



### **LSTM超参数调整**

以下是手动优化RNN超参数时需要注意的一些情况：

- 小心过拟合，神经网络基本在“记忆”训练数据时，就会发生过拟合。过拟合意味着你在训练数据上有很好的表现，在其他数据集上基本无用。
- 正则化有好处：方法包括 l1、 l2和dropout等。
- 要有一个单独的测试集，不要在这个测试集上训练网络。
- 网络越大，功能就越强，但也更容易过拟合。 不要试图从10000个示例中学习一百万个参数，参数>样例=麻烦。
- 数据越多越好，因为它有助于防止过度拟合。
- 训练要经过多个epoch(算法遍历训练数据集)。
- 每个epoch之后，评估测试集表现，以了解何时停止(要提前停止)。
- 学习速率是最重要的超参数。
- 总体而言，堆叠层会有帮助。
- 对于LSTM，可以使用softsign(而不是softmax)函数替代双曲正切函数，它更快，更不容易饱和( 梯度大概为0 )。
- 更新器：RMSProp、AdaGrad或Nesterovs通常是不错的选择。AdaGrad也会降低学习率，这有时会有所帮助。
- 记住，要将数据标准化、MSE损失函数+恒等激活函数用于回归、Xavier权重初始化



## 课后习题

> 1.关于循环神经网络描述错误的是：
>
> 1. 在同一个批量中，处理不同语句用到的模型参数W_{h}*W**h*和b_{h}*b**h*是一样的
> 2. 循环神经网络处理一个长度为T*T*的输入序列，需要维护T*T*组模型参数
> 3. 各个时间步的隐藏状态H_{t}*H**t*不能并行计算
> 4. 可以认为第t**个时间步的隐藏状态H_t**包含截止到第t个时间步的序列的历史信息



> 4.关于采样方法和隐藏状态初始化的描述错误的是：
>
> 1. 采用的采样方法不同会导致隐藏状态初始化方式发生变化
> 2. 采用相邻采样仅在每个训练周期开始的时候初始化隐藏状态是因为相邻的两个批量在原始数据上是连续的
> 3. 采用随机采样需要在每个小批量更新前初始化隐藏状态是因为每个样本包含完整的时间序列信息



### 答案解释

> 1. 答案解释
>
> 选项1：批量训练的过程中，参数是以批为单位更新的，每个批次内模型的参数都是一样的。
>
> 选项2：循环神经网络通过不断循环使用同样一组参数来应对不同长度的序列，故网络的参数数量与输入序列长度无关。
>
> 选项3：隐状态H_t*H**t*的值依赖于H_1, ..., H_{t-1}*H*1,...,*H**t*−1，故不能并行计算。
>
> 选项4：可以这么认为



> 4. 答案解释
>
> 选项1：正确，
>
> 选项2：正确，
>
> 选项3：错误，随机采样中每个样本只包含局部的时间序列信息，因为样本不完整所以每个批量需要重新初始化隐藏状态。



## Reference

[一文搞懂RNN（循环神经网络）基础篇 - 忆臻的文章 - 知乎](https://zhuanlan.zhihu.com/p/30844905)

[循环神经网络（RNN）和LSTM初学者指南 | 入门资料 - 量子位的文章 - 知乎](https://zhuanlan.zhihu.com/p/55671493)

[CNN（卷积神经网络）、RNN（循环神经网络）、DNN（深度神经网络）的内部网络结构有什么区别？ - 科言君的回答 - 知乎](https://www.zhihu.com/question/34681168/answer/84061846)